{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alejogiley/ChemGraphs/blob/prototype/notebooks/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPwXmOxKK0Su",
        "outputId": "320ed629-e891-4620-d764-ea9e4a7d91b8"
      },
      "source": [
        "%%bash\n",
        "\n",
        "url='https://raw.githubusercontent.com/alejogiley/ChemGraphs/prototype/datasets/estrogen_receptor_alpha.sdf'\n",
        "curl $url --output estrogen_receptor_alpha.sdf "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\r  5 34.6M    5 1844k    0     0   365k      0  0:01:37  0:00:05  0:01:32  370k\r100 34.6M  100 34.6M    0     0  6213k      0  0:00:05  0:00:05 --:--:-- 8092k\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm2ocr1791OM",
        "outputId": "4728a3bd-507b-42e9-81b5-56583b14b3eb"
      },
      "source": [
        "%%bash\n",
        "\n",
        "x86='/usr/lib/x86_64-linux-gnu'\n",
        "url='https://anaconda.org/rdkit/rdkit/2018.09.1.0/download/linux-64/rdkit-2018.09.1.0-py36h71b666b_1.tar.bz2'\n",
        "\n",
        "# download & extract\n",
        "curl -L $url | tar xj lib\n",
        "\n",
        "# move to python packages directory\n",
        "mv lib/python3.6/site-packages/rdkit /usr/local/lib/python3.6/dist-packages/\n",
        "mv lib/*.so.* $x86/\n",
        "\n",
        "# rdkit need libboost\n",
        "ln -s $x86/libboost_python3-py36.so.1.65.1 $x86/libboost_python3.so.1.65.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3845    0  3845    0     0   4188      0 --:--:-- --:--:-- --:--:--  4183\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r  0 20.2M    0  117k    0     0  52040      0  0:06:47  0:00:02  0:06:45  118k\r 21 20.2M   21 4447k    0     0  1362k      0  0:00:15  0:00:03  0:00:12 2287k\r 41 20.2M   41 8639k    0     0  2025k      0  0:00:10  0:00:04  0:00:06 2933k\r 61 20.2M   61 12.3M    0     0  2385k      0  0:00:08  0:00:05  0:00:03 3175k\r 82 20.2M   82 16.6M    0     0  2716k      0  0:00:07  0:00:06  0:00:01 3440k\r100 20.2M  100 20.2M    0     0  2955k      0  0:00:07  0:00:07 --:--:-- 4384k\n",
            "mv: cannot move 'lib/python3.6/site-packages/rdkit' to '/usr/local/lib/python3.6/dist-packages/rdkit': Directory not empty\n",
            "ln: failed to create symbolic link '/usr/lib/x86_64-linux-gnu/libboost_python3.so.1.65.1': File exists\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8yo69a-_5lN"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz_dCAvc_yEA"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install spektral"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpNOLnf-nvtU"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.sparse as sp\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "#from rdkit.Chem.Draw import IPythonConsole\n",
        "#from rdkit.Chem import Draw\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Input, \n",
        "    Activation, Dropout,\n",
        "    BatchNormalization)\n",
        "\n",
        "from spektral.data import BatchLoader, Dataset, Graph\n",
        "from spektral.transforms import LayerPreprocess\n",
        "from spektral.layers import (\n",
        "    ECCConv, GCSConv, \n",
        "    MinCutPool, GlobalSumPool)\n",
        "\n",
        "#IPythonConsole.ipython_useSVG=True  #< set this to False if you want PNGs instead of SVGs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8mf7FtZ-dK"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-w2A1TMnvtd"
      },
      "source": [
        "def ohc(x, keys):\n",
        "    maps = dict([(k, v) for k, v in zip(keys, range(len(keys)))])\n",
        "    return to_categorical(maps[x], num_classes=len(keys))\n",
        "\n",
        "def get_nodes(mol):\n",
        "    \"\"\"\n",
        "    the atomic numbers in this dataset\n",
        "    {5, 6, 7, 8, 9, 14, 15, 16, 17, 35, 53, 78}\n",
        "    so the on-hot-encoding would be of length 12\n",
        "    *this is temporary\n",
        "\n",
        "    \"\"\"\n",
        "    keys = [5, 6, 7, 8, 9, 14, 15, 16, 17, 35, 53, 78]\n",
        "\n",
        "    # nodes = np.concatenate((\n",
        "    #     np.array([(\n",
        "    #         ohc(atom.GetAtomicNum()), \n",
        "    #         atom.GetDoubleProp(\"_GasteigerCharge\"),\n",
        "    #         atom.atom.GetDegree())\n",
        "    #     for atom in mol.GetAtoms()]),\n",
        "    #     mol.GetConformer().GetPositions()[:,:2]),\n",
        "    #     axis=1\n",
        "    # )\n",
        "    AllChem.ComputeGasteigerCharges(mol)\n",
        "    \n",
        "    nodes = np.array([( \n",
        "        *ohc(atom.GetAtomicNum(), keys).tolist(),\n",
        "        atom.GetDoubleProp(\"_GasteigerCharge\"),\n",
        "        atom.GetDegree())\n",
        "        for atom in mol.GetAtoms()\n",
        "    ])\n",
        "\n",
        "    return nodes\n",
        "\n",
        "def symmetrize(matrix):\n",
        "    return matrix \\\n",
        "        + np.transpose(matrix, (1, 0, 2)) \\\n",
        "        - np.diag(matrix.diagonal())\n",
        "\n",
        "def get_edges(mol):\n",
        "    \"\"\"\n",
        "    Same with nodes, the bond types here are\n",
        "    {'AROMATIC', 'DOUBLE', 'SINGLE', 'TRIPLE'}\n",
        "    but the number of classes in rdkit is larger\n",
        "    *temporary solution\n",
        "\n",
        "    \"\"\"\n",
        "    keys = ['AROMATIC', 'DOUBLE', 'SINGLE', 'TRIPLE']\n",
        "\n",
        "    natms = mol.GetNumAtoms()\n",
        "    edges = np.zeros((natms, natms, len(keys)))\n",
        "    \n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edges[i, j] = ohc(str(bond.GetBondType()), keys)\n",
        "    \n",
        "    return symmetrize(edges)\n",
        "\n",
        "def str_is_float(s):\n",
        "    \n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    \n",
        "    except ValueError:\n",
        "        pass\n",
        " \n",
        "    try:\n",
        "        import unicodedata\n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    \n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        " \n",
        "    return False\n",
        "\n",
        "def get_labels(mol, key='IC50 (nM)'):\n",
        "    \"\"\"Generate label data for each molecule\n",
        "    \n",
        "    \"rigth\" and \"left\" indicates whether value is right-censored \">\"\n",
        "    or lef-censored \"<\" which are reported for concentrations beyond \n",
        "    detection limits.\n",
        "    \n",
        "    \"conc\" containts the reported concentration values\n",
        "    angle brackets are removed and boundary values are saved.\n",
        "    when conc value is 0, it means metric was not reported.\n",
        "    \n",
        "    \"\"\"\n",
        "    # read potency metric\n",
        "    sample = mol.GetPropsAsDict()[key]\n",
        "    # remove leading and trailing whitespaces\n",
        "    sample = sample.strip()\n",
        "        \n",
        "    # below exp. range\n",
        "    if \"<\" in sample: \n",
        "        \n",
        "        left = 1\n",
        "        right = 0\n",
        "        \n",
        "        conc = sample.replace('<', '')\n",
        "        conc = float(conc)\n",
        "\n",
        "    # outside exp. range\n",
        "    elif \">\" in sample:\n",
        "        \n",
        "        left = 0\n",
        "        right = 1\n",
        "        \n",
        "        conc = sample.replace('>', '')\n",
        "        conc = float(conc)\n",
        "\n",
        "    # inside exp. range\n",
        "    elif str_is_float(sample):\n",
        "        \n",
        "        left = 0\n",
        "        right = 0 \n",
        "        \n",
        "        conc = sample\n",
        "        conc = float(conc)\n",
        "\n",
        "    # no data provided\n",
        "    else:\n",
        "        \n",
        "        left = 0\n",
        "        right = 0\n",
        "        conc = 0.0\n",
        "    \n",
        "    return np.array([left, right, conc])"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZYsl4ELOn0d"
      },
      "source": [
        "# create instance of sdf reader\n",
        "#suppl = Chem.SDMolSupplier('estrogen_receptor_alpha.sdf', sanitize=True, strictParsing=True)\n",
        "\n",
        "# read all molecules besides ones with errors into a list\n",
        "#mols = [mol for mol in suppl if mol is not None]\n",
        "\n",
        "# Get nodes\n",
        "x = [get_nodes(mol) for mol in mols]\n",
        "    \n",
        "# Adjacency matrices\n",
        "a = [Chem.rdmolops.GetAdjacencyMatrix(mol) for mol in mols]\n",
        "\n",
        "# Edge features: bond types\n",
        "e = [get_edges(mol) for mol in mols]\n",
        "\n",
        "# Labels: (rank, IC50s)\n",
        "# this metric is less reliable than e.g. Kd as \n",
        "# it depends on the of the substrates used in \n",
        "# the essay and it is cell type dependent.\n",
        "y = [get_labels(mol) for mol in mols]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uXBHJI7nvtf"
      },
      "source": [
        "class EstrogenDB(Dataset):\n",
        "    \"\"\"Dataset from BindingDB\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 n_samples=1000,\n",
        "                 dpath=None, \n",
        "                 nodes=None, \n",
        "                 edges=None,\n",
        "                 adjcs=None, \n",
        "                 feats=None,\n",
        "                 **kwargs):\n",
        "        self.n_samples = n_samples\n",
        "        self.nodes = nodes\n",
        "        self.edges = edges\n",
        "        self.adjcs = adjcs\n",
        "        self.feats = feats\n",
        "        # dataset to load\n",
        "        self.dpath = dpath\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "\t\n",
        "    @Dataset.path.getter\n",
        "    def path(self):\n",
        "\t    path = os.path.join(self.dpath, f'EstrogenDB.npz')\n",
        "\t    return '' if not os.path.exists(path) else path\n",
        "\t        \n",
        "    def read(self):\n",
        "        # create Graph objects\n",
        "        data = np.load(\n",
        "            os.path.join(\n",
        "                self.dpath, \n",
        "                f'EstrogenDB.npz'), \n",
        "            allow_pickle=True)\n",
        "        \n",
        "        # size = len(data['y'])\n",
        "        \n",
        "        output = [\n",
        "            self.make_graph(\n",
        "                node=data['x'][i],\n",
        "                adjc=data['a'][i], \n",
        "                edge=data['e'][i],\n",
        "                feat=data['y'][i])\n",
        "            for i in range(self.n_samples)\n",
        "            if data['y'][i][-1] > 0\n",
        "        ]\n",
        "        \n",
        "        self.n_samples = len(output)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def download(self):\n",
        "        # save graph arrays into directory\n",
        "        filename = os.path.join(self.dpath, f'EstrogenDB')\n",
        "        \n",
        "        np.savez_compressed(\n",
        "            filename, \n",
        "            x=self.nodes, \n",
        "            a=self.adjcs, \n",
        "            e=self.edges, \n",
        "            y=self.feats)\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_graph(node, adjc, edge, feat):\n",
        "        # The node features\n",
        "        x = node.astype(float)\n",
        "        \n",
        "        # The adjacency matrix\n",
        "        # convert to scipy.sparse matrix\n",
        "        a = adjc.astype(int)\n",
        "        a = sp.csr_matrix(a)\n",
        "        # check shape (n_nodes, n_nodes)\n",
        "        assert a.shape[0] == len(node)\n",
        "        assert a.shape[1] == len(node)\n",
        "        \n",
        "        # The labels\n",
        "        y = feat.astype(float)\n",
        "        # transform IC50 values\n",
        "        # into pIC50\n",
        "        y[-1] = np.log10(y[-1])\n",
        "        \n",
        "        # The edge features \n",
        "        e = edge.astype(float)\n",
        "        # check shape (n_nodes, n_nodes, ..)\n",
        "        assert e.shape[0] == len(node)\n",
        "        assert e.shape[1] == len(node)\n",
        "        \n",
        "        return Graph(x=x, a=a, e=e, y=y)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhDKKqITnvtf",
        "outputId": "533b2806-1f1f-4a61-dfe2-b4f4ba2f74f7"
      },
      "source": [
        "%%time\n",
        "url = \"/content/\"\n",
        "\n",
        "dataset = EstrogenDB(\n",
        "    n_samples=2000,\n",
        "    #nodes=x, edges=e, \n",
        "    #adjcs=a, feats=y, \n",
        "    dpath=url)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 28min 22s, sys: 53 s, total: 29min 15s\n",
            "Wall time: 29min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvsF6XID5IM6",
        "outputId": "bf546195-f57b-446f-f42e-7e49533d5575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EstrogenDB(n_graphs=887)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SYHcgUtnvth"
      },
      "source": [
        "# Transform the adjacency matrix \n",
        "# according to ECCConv\n",
        "dataset.apply(LayerPreprocess(ECCConv))\n",
        "\n",
        "# randomize indexes\n",
        "indxs = np.random.permutation(len(dataset))\n",
        "\n",
        "# split 90%/10%\n",
        "split = int(0.9 * len(dataset))\n",
        "\n",
        "# Train/test indexes\n",
        "trnxs, tesxs = np.split(indxs, [split])\n",
        "\n",
        "# Dataset partition\n",
        "train, tests = dataset[trnxs], dataset[tesxs]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4duo2Minvtj"
      },
      "source": [
        "# class SimpleDense(Layer):\n",
        "\n",
        "#   def __init__(self, units=32):\n",
        "#       super(SimpleDense, self).__init__()\n",
        "#       self.units = units\n",
        "\n",
        "#   def build(self, input_shape):\n",
        "#       self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "#                                initializer='random_normal',\n",
        "#                                trainable=True)\n",
        "#       self.b = self.add_weight(shape=(self.units,),\n",
        "#                                initializer='random_normal',\n",
        "#                                trainable=True)\n",
        "\n",
        "#   def call(self, inputs):\n",
        "#       return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# def gcnn_model(nodes_shape, edges_shape, channels, n_layers, n_neurons):\n",
        "    \n",
        "#     X = Input(shape=(None, nodes_shape))\n",
        "#     A = Input(shape=(None, None))\n",
        "#     E = Input(shape=(None, None, edges_shape))\n",
        "\n",
        "#     y = ECCConv(n_channels)([X, A, E])\n",
        "#     y = Activation('relu')(y)\n",
        "    \n",
        "#     for i in range(1, n_layers):\n",
        "#         y = ECCConv(n_channels)([y, A, E])\n",
        "#         y = BatchNormalization(renorm=True)(y)\n",
        "#         y = Activation('relu')(y)\n",
        "    \n",
        "#     # dense block\n",
        "#     y = GlobalSumPool()(y)\n",
        "#     y = Dense(n_neurons)(y)\n",
        "#     y = Activation('relu')(y)\n",
        "#     y = Dropout(0.25)(y)\n",
        "#     y = Dense(1)(y)\n",
        "    \n",
        "#     # prediction\n",
        "#     y = Dense(1)(y)\n",
        "    \n",
        "#     return Model(inputs=[X, A, E], outputs=O)\n",
        "\n",
        "\n",
        "# def msent_loss(y_true, y_pred):\n",
        "    \n",
        "#     c_true, c_pred = y_true[:, 0], y_pred[:, 1:]\n",
        "#     p_true, p_pred = y_true[:, 1], y_pred[:, :1]\n",
        "    \n",
        "#     # categorical cross-entropy for classes: 0, 1, 2\n",
        "#     ent = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  \n",
        "#     # regression error for pIC50 values\n",
        "#     mse = tf.keras.losses.MeanSquaredError()\n",
        "    \n",
        "#     # return the overal error\n",
        "#     return tf.reduce_mean(\n",
        "#         ent(c_true, c_pred) + mse(p_true, p_pred))\n",
        "\n",
        "\n",
        "# def train_model(dataset, epochs, learning_rate, n_channels, n_layers, n_neurons): \n",
        "    \n",
        "#     # Parameters\n",
        "#     F = dataset.n_node_features  # Dimension of node features\n",
        "#     S = dataset.n_edge_features  # Dimension of edge features\n",
        "\n",
        "#     # Create GCN model\n",
        "#     model = gcn_model(\n",
        "#         nodes_shape=F, \n",
        "#         edges_shape=S, \n",
        "#         n_layers=n_layers, \n",
        "#         n_neurons=n_neurons,\n",
        "#         n_channels=n_channels)\n",
        "    \n",
        "#     # Compile GCN\n",
        "#     model.compile(\n",
        "#         optimizer=Adam(lr=learning_rate), \n",
        "#         #metrics=[\"mae\"],\n",
        "#         loss=msent_loss)\n",
        "    \n",
        "#     # Print network summary\n",
        "#     model.summary()\n",
        "    \n",
        "#     loader = BatchLoader(\n",
        "#         dataset, \n",
        "#         batch_size=batch_size)\n",
        "    \n",
        "#     # Trains the model\n",
        "#     history = model.fit(\n",
        "#         loader.load(),\n",
        "#         epochs=epochs,\n",
        "#         steps_per_epoch=loader.steps_per_epoch)\n",
        "    \n",
        "#     return model, history"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL9lbwf2I64r"
      },
      "source": [
        "def tobit_loss(y_true, y_pred, sigma, eps=1e-7):\n",
        "\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "    \n",
        "    # indicators of left-, right-censoring\n",
        "    y_lefts = y_true[:, 0]\n",
        "    y_right = y_true[:, 1]\n",
        "    y_value = y_true[:, 2]\n",
        "    \n",
        "    # normal distribution\n",
        "    normal = tfp.distributions.Normal(loc=0., scale=1.)\n",
        "    \n",
        "    # probability function of normal distribution at point y_value\n",
        "    prob = normal.prob((y_value - y_pred) / sigma) / sigma\n",
        "    # probability of point random variable being > than y_value\n",
        "    right_prob = 1 - normal.cdf((y_value - y_pred) / sigma)\n",
        "    # probability of random variable being < than y_value\n",
        "    lefts_prob = normal.cdf((y_value - y_pred) / sigma)\n",
        "    \n",
        "    # clip tensor values\n",
        "    prob = tf.clip_by_value(\n",
        "        prob, \n",
        "        clip_value_min=eps, \n",
        "        clip_value_max=1/eps)\n",
        "    \n",
        "    right_prob = tf.clip_by_value(\n",
        "        right_prob, \n",
        "        clip_value_min=eps, \n",
        "        clip_value_max=1/eps)\n",
        "        \n",
        "    left_prob = tf.clip_by_value(\n",
        "        lefts_prob, \n",
        "        clip_value_min=eps, \n",
        "        clip_value_max=1/eps)\n",
        "    \n",
        "    # logarithm of likelihood\n",
        "    logp = tf.math.log(prob) * (1 - y_right) * (1 - y_lefts) \\\n",
        "           + tf.math.log(right_prob) * y_right * (1 - y_lefts) \\\n",
        "           + tf.math.log(lefts_prob) * y_lefts * (1 - y_right)\n",
        "    \n",
        "    return - tf.reduce_sum(logp)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # indicators of left-, right-censoring\n",
        "    y_lefts = y_true[:, 0]\n",
        "    y_right = y_true[:, 1]\n",
        "    y_value = y_true[:, 2]\n",
        "\n",
        "    delta = (y_value - y_pred) #* (1 - y_right) * (1 - y_lefts)\n",
        "    return tf.reduce_sum(tf.square(delta))\n",
        "\n",
        "def train_gcnn(dataset, epochs, learning_rate, channels, n_layers, n_neurons): \n",
        "    \n",
        "    # Create GCN model\n",
        "    model = GCNN(\n",
        "        channels=channels,\n",
        "        n_layers=n_layers, \n",
        "        n_neurons=n_neurons)\n",
        "    \n",
        "    # Loader returns batches of graphs\n",
        "    # with zero-padding done batch-wise\n",
        "    loader = BatchLoader(\n",
        "        dataset, batch_size=batch_size, epochs=epochs)\n",
        "    \n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "    # Time-based learning rate schedule\n",
        "    decay_step = 1.0\n",
        "    decay_rate = learning_rate / epochs\n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "        learning_rate, decay_step, decay_rate)\n",
        "    optimizer = SGD(learning_rate==learning_rate_fn)\n",
        "    \n",
        "    @tf.function(\n",
        "        input_signature=loader.tf_signature(), \n",
        "        experimental_relax_shapes=True)\n",
        "    def train_step(inputs, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            #predictions, sigma = model(inputs, training=True)\n",
        "            #loss = tobit_loss(targets, predictions, sigma)\n",
        "            predictions = model(inputs, training=True)\n",
        "            loss = mse_loss(targets, predictions)\n",
        "            loss += sum(model.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        return loss\n",
        "        #train_loss(loss)\n",
        "    \n",
        "    # Train model\n",
        "    print(\"Fitting model\")\n",
        "    model_lss = 0\n",
        "    for k, batch in enumerate(loader):\n",
        "        \n",
        "        #train_loss.reset_states()\n",
        "        loss = train_step(*batch)\n",
        "        model_lss += loss.numpy()\n",
        "\n",
        "        if k % loader.steps_per_epoch == 0:\n",
        "            model_lss /= loader.steps_per_epoch\n",
        "            print(\"Epoch {}. Loss: {}\".format(\n",
        "                k // loader.steps_per_epoch, model_lss))\n",
        "            model_lss = 0\n",
        "\n",
        "    return model\n",
        "\n",
        "class GCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, channels, n_layers, n_neurons, **kwargs):\n",
        "        super(GCNN, self).__init__()\n",
        "        \n",
        "        # initialize dense layers\n",
        "        self.dense1 = Dense(n_neurons)\n",
        "        self.dense2 = Dense(1)\n",
        "\n",
        "        # initialize operations\n",
        "        self.activation = Activation('relu')\n",
        "        self.dropout = Dropout(0.15)\n",
        "        self.pooling = GlobalSumPool()\n",
        "        self.batchnm = BatchNormalization(renorm=True)\n",
        "        \n",
        "        # initialize edge-conditioned convolutional layers\n",
        "        self.conv1 = ECCConv(channels, activation=\"relu\")\n",
        "        self.convs = []\n",
        "        for i in range(1, n_layers):\n",
        "            self.convs.append(ECCConv(channels, activation=\"relu\"))\n",
        "\n",
        "        # last layer linear model: y = ax + b\n",
        "        # self.a = tf.Variable(tf.ones([n_neurons, 1]), trainable=True)\n",
        "        # self.b = tf.Variable(1., trainable=True)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x, a, e = inputs\n",
        "\n",
        "        #x = tf.cast(x, tf.float32)\n",
        "        a = tf.cast(a, tf.float32)\n",
        "        #e = tf.cast(e, tf.float32)\n",
        "        \n",
        "        x = self.conv1([x, a, e])\n",
        "        #x = self.activation(x)\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            x = conv([x, a, e])\n",
        "            #x = self.batchnm(x)\n",
        "            #x = self.activation(x)\n",
        "            # x = self.dropout(x)\n",
        "        \n",
        "        x = self.pooling(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "        #return tf.matmul(x, self.a) + self.b\n",
        "        # return tf.matmul(x, self.a), self.b"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-7wElUznvti"
      },
      "source": [
        "epochs = 6  # Number of training epochs\n",
        "batch_size = 32 # MiniBatch sizes\n",
        "learning_rate = 1e-3 # Optimizer learning rate\n",
        "\n",
        "n_layers = 12  # number of ECCConv layers\n",
        "n_neurons = 8  # number of Dense channels\n",
        "n_channels = 32  # number of Hidden units"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4aHUtt_nvtj",
        "outputId": "2e7b7265-d2f7-4d49-857b-d35de1000739"
      },
      "source": [
        "model = train_gcnn(train, epochs, learning_rate, n_channels, n_layers, n_neurons)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting model\n",
            "Epoch 0. Loss: 20.5051318359375\n",
            "Epoch 1. Loss: 387.0805334472656\n",
            "Epoch 2. Loss: 397.7018933105469\n",
            "Epoch 3. Loss: 392.12903198242185\n",
            "Epoch 4. Loss: 399.5705310058594\n",
            "Epoch 5. Loss: 396.4114196777344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoWfANBDO9n_"
      },
      "source": [
        "model.a.value(), model.b.value()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vDcqg8xaDPi"
      },
      "source": [
        "dataset[3]['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myZnKO-ynvtk"
      },
      "source": [
        "print(\"Testing model\")\n",
        "loader = BatchLoader(tests, batch_size=batch_size, epochs=1, shuffle=False)\n",
        "\n",
        "model_loss = 0.0\n",
        "for batch in loader:\n",
        "    inputs, target = batch\n",
        "    predictions= model(inputs, training=False)\n",
        "    print(predictions)\n",
        "    model_loss += mse_loss(target, predictions)\n",
        "\n",
        "model_loss /= loader.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(model_loss))\n",
        "\n",
        "# print(\"Testing model\")\n",
        "# loader = BatchLoader(tests, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# model_loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)\n",
        "# print(\"Done. Test loss: {}\".format(model_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJC3MhBRG-q"
      },
      "source": [
        "[test.y for test in tests]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyxf6yelSfK0"
      },
      "source": [
        "tobit_loss(target, predictions, sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5J4tD7oSfRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQGtczSZ90gi"
      },
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "prediction = model.predict(loader.load(), steps=loader.steps_per_epoch)\n",
        "\n",
        "pIC50_true = [tests[i]['y'][1] for i in range(tests.n_graphs)]\n",
        "class_true = [tests[i]['y'][0] for i in range(tests.n_graphs)] \n",
        "\n",
        "pIC50_pred = prediction[:, :1]\n",
        "class_pred = np.argmax(np.apply_along_axis(softmax, 0, prediction[:, 1:]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krEH_O_V90gj"
      },
      "source": [
        "pIC50_true, pIC50_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAZaEgz790gk"
      },
      "source": [
        "class_true, class_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXmY_lOO90gk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b39RgRB390gl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}